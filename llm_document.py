# -*- coding: utf-8 -*-
"""LLM_Document

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nxARJE3eLpzrUrZ7Sn8rPSMST6WYeW30
"""

!pip install -U langchain-community
from langchain_community.document_loaders import PyPDFLoader, TextLoader

!pip install -q langchain openai faiss-cpu sentence-transformers pypdf
from google.colab import files

uploaded = files.upload()  # Upload your PDF file here
file_path = list(uploaded.keys())[0]
print(f"Uploaded file: {file_path}")
from langchain.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Use appropriate loader based on file extension
if file_path.endswith(".pdf"):
    loader = PyPDFLoader(file_path)
else:
    loader = TextLoader(file_path)

docs = loader.load()

# Split the text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)
print(f"Number of text chunks: {len(chunks)}")
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import FAISS

embedding_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
db = FAISS.from_documents(chunks, embedding_model)
import os
from langchain.chat_models import ChatOpenAI

os.environ["OPENAI_API_KEY"] = "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"

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=db.as_retriever(),
    chain_type="stuff"
)

# Ask questions
while True:
    query = input("Ask a question about your document (or type 'exit'): ")
    if query.lower() == 'exit':
        break
    response = qa.run(query)
    print("\nðŸ“˜ Answer:", response)

# Install required packages
!pip install -q langchain langchain-community langchain-openai faiss-cpu sentence-transformers pypdf

from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.chains import RetrievalQA
from google.colab import files
import os

# Securely input OpenAI API key (avoid hardcoding)
try:
    from google.colab import userdata
    os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
except ImportError:
    # Fallback for non-Colab environments or manual input
    os.environ["OPENAI_API_KEY"] = input("Please enter your OpenAI API key: ")

# Upload file
try:
    print("Please upload your PDF or text file.")
    uploaded = files.upload()
    if not uploaded:
        raise ValueError("No file uploaded.")
    file_path = list(uploaded.keys())[0]
    print(f"Uploaded file: {file_path}")
except Exception as e:
    print(f"Error during file upload: {e}")
    exit()

# Load document based on file extension
try:
    if file_path.endswith(".pdf"):
        loader = PyPDFLoader(file_path)
    elif file_path.endswith(".txt"):
        loader = TextLoader(file_path)
    else:
        raise ValueError("Unsupported file format. Please upload a .pdf or .txt file.")
    docs = loader.load()
    if not docs:
        raise ValueError("No content loaded from the document.")
except Exception as e:
    print(f"Error loading document: {e}")
    exit()

# Split the text into chunks
try:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(docs)
    print(f"Number of text chunks: {len(chunks)}")
    if not chunks:
        raise ValueError("No chunks created from the document.")
except Exception as e:
    print(f"Error splitting document: {e}")
    exit()

# Create embeddings and vector store
try:
    embedding_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
    db = FAISS.from_documents(chunks, embedding_model)
except Exception as e:
    print(f"Error creating vector store: {e}")
    exit()

# Initialize the LLM
try:
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
except Exception as e:
    print(f"Error initializing LLM: {e}")
    exit()

# Set up RetrievalQA chain
try:
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=db.as_retriever(),
        chain_type="stuff"
    )
except Exception as e:
    print(f"Error setting up QA chain: {e}")
    exit()

# Ask questions
print("You can now ask questions about your document. Type 'exit' to stop.")
while True:
    query = input("Ask a question about your document (or type 'exit'): ")
    if query.lower() == 'exit':
        print("Exiting...")
        break
    try:
        response = qa.invoke(query)
        print("\nðŸ“˜ Answer:", response['result'])
    except Exception as e:
        print(f"Error processing query: {e}")